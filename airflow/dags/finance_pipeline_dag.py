# dags/jsonl_to_es_dag.py

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
from elasticsearch import Elasticsearch, helpers
from sentence_transformers import SentenceTransformer
import os
import glob
import json
import numpy as np  # nan/inf ì²´í¬ìš©

# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ 1. ì „ì—­ ì„¤ì •                                   â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

# Elasticsearch ì¸ë±ìŠ¤ëª…
INDEX_NAME    = "finance_knowledge_chunks"

# JSONL íŒŒì¼ì„ ëª¨ì•„ë‘˜ ë””ë ‰í† ë¦¬ ê²½ë¡œ
JSONL_FOLDER  = "/data/"

# EC2ì— ë°°í¬ëœ Elasticsearch URL
ES_URL        = "http://52.78.209.43:9200/"

# ì¸ì¦ì„œ ê²€ì¦ ì—¬ë¶€ (Falseë¡œ í•˜ë©´ HTTPS ì¸ì¦ì„œ ë¬´ì‹œ)
VERIFY_CERTS  = False

# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ 2. DAG ê¸°ë³¸ ì¸ìˆ˜                              â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

DEFAULT_ARGS = {
    "owner":            "finance_team",               # íƒœìŠ¤í¬ ì†Œìœ ìž
    "depends_on_past":  False,                        # ì´ì „ ì‹¤í–‰ ê²°ê³¼ì— ì˜ì¡´í•˜ì§€ ì•ŠìŒ
    "start_date":       datetime(2025, 5, 27),        # ìŠ¤ì¼€ì¤„ ì‹œìž‘ì¼
    "retries":          1,                            # ì‹¤íŒ¨ ì‹œ ìž¬ì‹œë„ íšŸìˆ˜
    "retry_delay":      timedelta(minutes=5),         # ìž¬ì‹œë„ ê°„ê²©
}

# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ 3. DAG ì •ì˜                                   â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

with DAG(
    dag_id="jsonl_folder_to_ec2_es",
    default_args=DEFAULT_ARGS,
    schedule_interval="@daily",  # ë§¤ì¼ í•œ ë²ˆ ì‹¤í–‰
    catchup=False                # ê³¼ê±° ì¼ìž ì±„ìš°ê¸° ë¹„í™œì„±í™”
) as dag:

    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚ 3-1. ì¸ë±ìŠ¤ ìƒì„± Task                        â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    def task_create_index(**context):
        """
        Elasticsearchì— ì ‘ì†í•˜ì—¬ INDEX_NAME ì¸ë±ìŠ¤ê°€ ì¡´ìž¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±í•œë‹¤.
        ì´ë¯¸ ì¡´ìž¬í•  ê²½ìš°, ê¸°ì¡´ ë§¤í•‘ì˜ embedding ì°¨ì›ì´ ëª¨ë¸ ì°¨ì›ê³¼ ë‹¤ë¥´ë©´
        ì¸ë±ìŠ¤ë¥¼ ì‚­ì œ í›„ ìž¬ìƒì„±í•œë‹¤.
        """
        # Elasticsearch í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (v11 í˜¸í™˜)
        es = Elasticsearch(
            ES_URL,
            verify_certs=VERIFY_CERTS,
            request_timeout=120,
            max_retries=3,
            retry_on_timeout=True
        )
        # SBERT ëª¨ë¸ ë¡œë”© ë° ìž„ë² ë”© ì°¨ì› ì·¨ë“
        model = SentenceTransformer("nlpai-lab/KURE-v1")
        MODEL_DIM = model.get_sentence_embedding_dimension()

        # ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ìž¬í•˜ëŠ”ì§€ í™•ì¸
        if es.indices.exists(index=INDEX_NAME):
            mapping_info = es.indices.get_mapping(index=INDEX_NAME)
            cur_dims = mapping_info[INDEX_NAME]['mappings']['properties']['embedding']['dims']
            if cur_dims != MODEL_DIM:
                print(f"âš ï¸ ê¸°ì¡´ dims({cur_dims}) != ëª¨ë¸ dims({MODEL_DIM}), ì¸ë±ìŠ¤ ìž¬ìƒì„±")
                es.indices.delete(index=INDEX_NAME)
            else:
                print(f"â„¹ï¸ ì¸ë±ìŠ¤ '{INDEX_NAME}' ì´ë¯¸ ì¡´ìž¬ (dims ì¼ì¹˜)")
                return

        # ë§¤í•‘ ì •ì˜ (ëª¨ë¸ ì°¨ì›ì— ë§žì¶¤)
        mapping = {
            "mappings": {
                "properties": {
                    "chunk_id":       {"type": "keyword"},
                    "page_id":        {"type": "integer"},
                    "title":          {"type": "keyword"},
                    "created_at":     {"type": "date"},
                    "source":         {"type": "keyword"},
                    "overlap_tokens": {"type": "integer"},
                    "text":           {"type": "text",
                                       "analyzer":"nori"},  # analyzer="nori"
                    "embedding": {
                        "type":           "dense_vector",
                        "dims":           MODEL_DIM,
                        "index":          True,
                        "similarity":     "cosine",
                        "index_options": {
                            "type":            "hnsw",
                            "m":               16,
                            "ef_construction": 100
                        }
                    }
                }
            }
        }

        # ì¸ë±ìŠ¤ ìƒì„±
        es.indices.create(index=INDEX_NAME, body=mapping)
        print(f"âœ… ì¸ë±ìŠ¤ '{INDEX_NAME}' ìƒì„± ì™„ë£Œ (dims={MODEL_DIM})")

    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚ 3-2. JSONL í´ë” ìˆœíšŒ ë° ì—…ë¡œë“œ Task           â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    def task_upload_folder(**context):
        """
        JSONL_FOLDER ë‚´ì˜ ëª¨ë“  .jsonl íŒŒì¼ì„ ì°¾ì•„,
        ë ˆì½”ë“œë¥¼ ì½ì–´ ìž„ë² ë”©ì„ ìƒì„±í•œ ë’¤ Elasticsearchì— ì—…ë¡œë“œí•œë‹¤.
        """
        es = Elasticsearch(
            ES_URL,
            verify_certs=VERIFY_CERTS,
            request_timeout=120,
            max_retries=3,
            retry_on_timeout=True
        )
        model = SentenceTransformer("nlpai-lab/KURE-v1")
        MODEL_DIM = model.get_sentence_embedding_dimension()

        # í´ë” ë‚´ ëª¨ë“  JSONL íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        pattern = os.path.join(JSONL_FOLDER, "*.jsonl")
        jsonl_files = glob.glob(pattern)

        if not jsonl_files:
            print(f"âŒ JSONL íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {JSONL_FOLDER}")
            return

        # ê° íŒŒì¼ ìˆœíšŒí•˜ë©° ì—…ë¡œë“œ
        for filepath in jsonl_files:
            print(f"ðŸ“„ ì—…ë¡œë“œ ì‹œìž‘: {filepath}")
            actions = []
            filename = os.path.basename(filepath)

            with open(filepath, "r", encoding="utf-8") as f:
                for i, line in enumerate(f):
                    doc = json.loads(line)
                    text = doc.get("text", "")

                    # ìž„ë² ë”© ìƒì„± ë° ì •ê·œí™”
                    embedding = model.encode(text, normalize_embeddings=True)

                    # ìœ íš¨ì„± ê²€ì‚¬: ì°¨ì› ë¶ˆì¼ì¹˜, NaN/Inf
                    if (len(embedding) != MODEL_DIM
                        or np.isnan(embedding).any()
                        or np.isinf(embedding).any()):
                        print(f"âŒ ìž˜ëª»ëœ ìž„ë² ë”© ê±´ë„ˆëœ€ (line {i}, len={len(embedding)})")
                        continue

                    doc["embedding"] = embedding.tolist()
                    unique_id = f"{filename}_{i:05d}"
                    actions.append({
                        "_op_type": "index",
                        "_index":   INDEX_NAME,
                        "_id":      unique_id,
                        "_source":  doc
                    })

                    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ Bulk ì „ì†¡
                    if len(actions) >= 500:
                        success, errors = helpers.bulk(
                            es, actions,
                            stats_only=False,
                            raise_on_error=False
                        )
                        print(f"ðŸ”„ Batch ì—…ë¡œë“œ: ì„±ê³µ {success}, ì‹¤íŒ¨ {len(errors)}ê±´")
                        actions.clear()

            # ìž”ì—¬ ì•¡ì…˜ ì „ì†¡
            if actions:
                success, errors = helpers.bulk(
                    es, actions,
                    stats_only=False,
                    raise_on_error=False
                )
                print(f"ðŸ”„ ìµœì¢… ì—…ë¡œë“œ: ì„±ê³µ {success}, ì‹¤íŒ¨ {len(errors)}ê±´")

            print(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {filepath}")

    # PythonOperatorë¡œ Task ë“±ë¡
    t1 = PythonOperator(
        task_id="create_index",
        python_callable=task_create_index,
        provide_context=True
    )
    t2 = PythonOperator(
        task_id="upload_folder",
        python_callable=task_upload_folder,
        provide_context=True
    )

    # ì‹¤í–‰ ìˆœì„œ: ì¸ë±ìŠ¤ ìƒì„± â†’ í´ë” ì—…ë¡œë“œ
    t1 >> t2
