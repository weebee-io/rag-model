from .base import RetrieverStrategy
from ..clients.elastic import es_client
from ..api.v1.schemas import Hit
from ..core.config import settings
import re

class TestBM25Retriever(RetrieverStrategy):

    def preprocess_query(self, query: str) -> str:
        # "Ï±ÑÍ∂åÏù¥ Î≠êÏïº?" ‚Üí "Ï±ÑÍ∂å"
        query = re.sub(r"[^\uAC00-\uD7A3a-zA-Z0-9\s]", "", query)  # ÌäπÏàòÎ¨∏Ïûê Ï†úÍ±∞
        query = re.sub(r"\b(Î≠êÏïº|ÏïåÎ†§Ï§ò|Ïù¥ÎûÄ|Ïù¥Í≤å|Ïñ¥Îïå)\b", "", query)  # ÏûêÏ£º Ïì∞Îäî ÏßàÎ¨∏Ìòï Ï°∞ÏÇ¨ Ï†úÍ±∞
        return query.strip()
    

    def is_text_clean(self, text: str, query_keywords: list[str]) -> bool:
        # Í≥µÎ∞± Ï†úÍ±∞Ìïú Ïã§Ï†ú ÌÖçÏä§Ìä∏ Í∏∏Ïù¥
        clean_text_len = len(re.sub(r"\s+", "", text))

        # ÌäπÏàòÎ¨∏Ïûê Ï∞æÍ∏∞ (ÌïúÍ∏Ä, ÏòÅÎ¨∏, Ïà´Ïûê, Í≥µÎ∞± Ï†úÏô∏)
        special_chars = re.findall(r"[^\uAC00-\uD7A3a-zA-Z0-9\s]", text)
        special_char_count = len(special_chars)

        # ÎîîÎ≤ÑÍπÖÏö© Ï∂úÎ†•
        if special_char_count / max(clean_text_len, 1) >= 0.2:
            print("üö´ ÌäπÏàòÎ¨∏Ïûê ÎπÑÏú® ÎÜíÏùå:", round(special_char_count / clean_text_len, 2), "| ÎÇ¥Ïö©:", text[:50])
            return False


        # ÏßàÏùò ÌÇ§ÏõåÎìúÍ∞Ä Ìè¨Ìï®ÎêòÏñ¥ ÏûàÏßÄ ÏïäÏúºÎ©¥ Ï†úÍ±∞
        if not any(keyword in text for keyword in query_keywords):
            return False

        return True

    def post_filter_hits(self, hits: list[dict], query_keywords: list[str], top_k: int = 3) -> list[dict]:
        filtered = [
            hit for hit in hits
            if self.is_text_clean(hit["_source"]["text"], query_keywords)
            ]
        return filtered[:top_k]

    async def search(self, query: str, top_k: int):
        # 1. Í≤ÄÏÉâÏñ¥ Ï†ÑÏ≤òÎ¶¨ (Ïòà: Ï°∞ÏÇ¨/Í∞êÌÉÑÏÇ¨/Î∂àÏö©Ïñ¥ Ï†úÍ±∞)
        query_clean = self.preprocess_query(query)

        # 2. Elasticsearch ÏøºÎ¶¨ Íµ¨ÏÑ± (nori_analyzer Í∏∞Î∞ò + ÌôïÏû•)
        body = {
            "size": top_k,
            "query": {
                "bool": {
                    "should": [
                        {
                            "match": {
                                "text": {
                                    "query": query_clean,
                                    "analyzer": "nori_analyzer",
                                    "boost": 2  # Í∞ÄÏ§ëÏπò Í∞ïÌôî
                                }
                            }
                        },
                        {
                            "match_phrase": {
                                "text": {
                                    "query": query_clean,
                                    "analyzer": "nori_analyzer",
                                    "boost": 1.5
                                }
                            }
                        },
                        {
                            "multi_match": {
                                "query": query_clean,
                                "fields": ["meta.keywords"],
                                "analyzer": "nori_analyzer",
                                "boost": 1
                            }
                        },
                            # Ïò§ÌÉÄ Í≤ÄÏÉâ Ï∂îÍ∞Ä (Fuzzy Matching)
                        {
                            "fuzzy": {
                                "text": {
                                    "value": query_clean,
                                    "fuzziness": "AUTO",
                                    "boost": 0.5  # ÎÇÆÏùÄ Í∞ÄÏ§ëÏπò
                                }
                            }
                        }
                    ],
                    "minimum_should_match": 2  # Îëê Ï°∞Í±¥ Ïù¥ÏÉÅ Îß§Ïπ≠ÎêòÎ©¥ ÌÜµÍ≥º
                }
            }
        }

        # 3. Í≤ÄÏÉâ ÏöîÏ≤≠
        resp = await es_client.search(index=settings.index_name, body=body)
        
        query_keywords = query_clean.split()
        filtered_hits = self.post_filter_hits(resp["hits"]["hits"], query_keywords, top_k)

        return [
            Hit(
                doc_id=hit["_id"],
                score=hit["_score"],
                source=hit["_source"]["source"],
                text=hit["_source"]["text"]
            )
            for hit in filtered_hits
        ]
